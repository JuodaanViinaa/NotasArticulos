\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[spanish]{babel}
\usepackage{csquotes}
\usepackage{anysize}
\usepackage{graphicx}
\marginsize{25mm}{25mm}{25mm}{25mm}

\title{Deviation from the matching law reflects an optimal strategy involving learning over multiple timescales}
\author{Kiyohito Iigaya \and Yashar Ahmadian \and Leo P. Sugrue \and Greg S. Corrado \and Yonatan Loewenstein \and William T. Newsome \and Stefano Fusi}
\date{2019}

\begin{document}
{\scshape\bfseries \maketitle}

Los modelos de refuerzo asumen un aprendizaje gradual. La estrategia óptima de aprendizaje dependerá de la volatilidad del ambiente: en un entorno volátil será beneficioso aprender rápidamente de experiencias recientes; en uno estable, aprender lentamente.

Se suele asumir que los animales ajustan una única tasa de aprendizaje para cambiar la velocidad a la que aprenden, pero se ha mostrado que los ambientes pueden cambiar en múltiples escalas temporales, por lo que aprender en múltiples escalas puede ser benéfico.

Se muestra que procesos de integración cooperativos en múltiples escalas pueden maximizar la recompensa obtenida por los animales. Los sujetos se desviaron de la conducta que sería óptima en un ambiente estacionario


\end{document}
